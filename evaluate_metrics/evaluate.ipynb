{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "804c8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "168a5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.insert(0, parent_dir)\n",
    "from services.scenario_service import ScenarioService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9909b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_data/ipc_scenarios.csv')    \n",
    "test_df['text_length'] = test_df['text'].apply(len)\n",
    "test_df = test_df.sort_values(by='text_length', ascending=True)\n",
    "test_df = test_df[test_df['text_length'] >= 1000]\n",
    "test_df = test_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9aef3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipc_to_bns = {}\n",
    "with open(\"test_data/ipc_bns_mapping.csv\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    import csv\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ipc = row[\"IPC\"].strip()\n",
    "        bns = row[\"BNS\"].strip()\n",
    "        ipc_to_bns[ipc] = bns\n",
    "\n",
    "def dcg_at_k(relevance_scores, k):\n",
    "    return sum(rel / np.log2(idx + 2) for idx, rel in enumerate(relevance_scores[:k]))\n",
    "\n",
    "def ndcg_at_k(predicted, ground_truth, k):\n",
    "    relevance_scores = [1 if sec in ground_truth else 0 for sec in predicted]\n",
    "    dcg = dcg_at_k(relevance_scores, k)\n",
    "    ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "    idcg = dcg_at_k(ideal_relevance, k)\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39d5d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scenarios(service, test_df, top_k=5):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Evaluating\"):\n",
    "        query = row[\"text\"]\n",
    "\n",
    "        # Clean and split section string\n",
    "        ipc_sections = [s.strip() for s in str(row[\"sections\"]).split(\",\") if s.strip().isdigit()]\n",
    "\n",
    "        # Map to BNS sections using your mapping\n",
    "        bns_mapped_sections = {ipc_to_bns[s] for s in ipc_sections if s in ipc_to_bns}\n",
    "        ground_truth = bns_mapped_sections\n",
    "        print(ground_truth)\n",
    "\n",
    "        if not ground_truth:\n",
    "            print(f\"⚠️ Skipping: No ground truth found for query: {query}\")\n",
    "            continue\n",
    "\n",
    "        # Get predictions\n",
    "        top_sections = service.get_top_scenarios(query, history=[], top_k=top_k, validate_with_api=False)\n",
    "        predicted = [str(sec[\"Section Number\"]) for sec in top_sections if \"Section Number\" in sec]\n",
    "        print(predicted)\n",
    "        if not predicted:\n",
    "            print(f\"⚠️ No predictions for query: {query}\")\n",
    "            precision = 0.0\n",
    "            recall = 0.0\n",
    "            ndcg = 0.0\n",
    "        else:\n",
    "            hits = sum(1 for sec in predicted if sec in ground_truth)\n",
    "            precision = hits / len(predicted) if predicted else 0.0\n",
    "            recall = hits / len(ground_truth) if ground_truth else 0.0\n",
    "            ndcg = ndcg_at_k(predicted, ground_truth, top_k)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "    total_evals = len(precision_scores)  # may be < len(test_df) due to skips\n",
    "\n",
    "    if total_evals == 0:\n",
    "        print(\"❌ No valid evaluations could be performed.\")\n",
    "        return {}, [], [], []\n",
    "\n",
    "    results = {\n",
    "        \"Mean Precision@k\": round(sum(precision_scores) / total_evals, 4),\n",
    "        \"Mean Recall@k\": round(sum(recall_scores) / total_evals, 4),\n",
    "        \"Mean NDCG@k\": round(sum(ndcg_scores) / total_evals, 4),\n",
    "    }\n",
    "\n",
    "    return results, precision_scores, recall_scores, ndcg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55c7f560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No valid evaluations could be performed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scenario_service = ScenarioService(dataset_path=\"../data/Updated_BNS_Dataset.csv\")\n",
    "results, precisions, recalls, mrrs = evaluate_scenarios(scenario_service, test_df, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
